# -*- coding: utf-8 -*-
"""HW 4 A

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1odD7hMIRr6mUDfz-WIsgtczK8S9Thk-Y
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("crowdflower/twitter-airline-sentiment")

print("Path to dataset files:", path)

###### PART A #####

import pandas as pd
df = pd.read_csv(path + "/Tweets.csv")
print(df.head())

relevant_columns = ["tweet_id", 'airline_sentiment', "retweet_count", 'negativereason','text','airline', 'name']

data = pd.read_csv(path + "/Tweets.csv", usecols=relevant_columns)
print(data.info())

sentiment_unique = data['airline_sentiment'].unique()
print(sentiment_unique)

sentiment_most_common = data['airline_sentiment'].mode()[0]
print(sentiment_most_common)

sentiment_frequency = data['airline_sentiment'].value_counts().iloc[0]
print(sentiment_frequency)

negativereason_unique = data['negativereason'].unique()
print(negativereason_unique)

negativereason_most_common = data['negativereason'].mode()[0]
print(negativereason_most_common)

negativereason_frequency = data['negativereason'].value_counts().iloc[0]
print(negativereason_frequency)

tweet_length= data['text'].str.len()
print(tweet_length)

shortest_length = tweet_length.min()
print(shortest_length)

longest_tweet = tweet_length.max()
print(longest_tweet)

import matplotlib.pyplot as plt
plt.hist(tweet_length, bins=20)
plt.xlabel('Tweet Length')
plt.ylabel('Frequency')
plt.title('Distribution of Tweet Lengths')
plt.show()

##### PART B ######

def plot_sentiment_distribution(df):
    airlines = df['airline'].unique()
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    colors = {'negative': 'red', 'neutral': 'gray', 'positive': 'green'}

    for idx, airline in enumerate(airlines):
        row = idx // 3
        col = idx % 3

        airline_data = df[df['airline'] == airline]
        sentiment_counts = airline_data['airline_sentiment'].value_counts()

        ax = axes[row, col]
        sentiment_counts.plot(kind='bar', ax=ax, color=[colors[sent] for sent in sentiment_counts.index])
        ax.set_title(airline)
        ax.set_ylabel('Count')
        ax.tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

plot_sentiment_distribution(data)

##### PART C ######

import re

def custom_tokenizer(text):
    # Tokenization rules using regular expressions
    rules = [
        # Rule 1: Split on whitespace and punctuation
        (r"\s+", " "),
        # Rule 2: Split on hashtags
        (r"#\w+", " "),
        # Rule 3: Split on mentions
        (r"@\w+", " "),
        # Rule 4: Split on URLs
        (r"http\S+", " "),
        # Rule 5: Split on emojis (simplified)
        (r"[\U0001F600-\U0001F64F]", " "),
    ]

    # Apply rules sequentially
    for pattern, replacement in rules:
        text = re.sub(pattern, replacement, text)

    # Split into individual tokens
    tokens = text.lower().split()

    return tokens

# Example usage
text = "This is a sample tweet with #hashtags, @mentions, and http://example.com ðŸ˜€"
tokens = custom_tokenizer(text)
print(tokens)

##### PART D ######

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

from nltk.tokenize import word_tokenize

# Function to find and print differences
def compare_tokenizers(text):
    custom_tokens = custom_tokenizer(text)
    nltk_tokens = word_tokenize(text)

    if custom_tokens != nltk_tokens:
        print("Original Text:", text)
        print("Custom Tokens:", custom_tokens)
        print("NLTK Tokens:", nltk_tokens)
        print("-" * 20)
        return True
    else:
        return False

# Open file to write the output
with open("tokenizer_comparison.txt", "w") as f:
    # Process the dataset and find differences
    count = 0
    for text in data['text']:
        if compare_tokenizers(text) and count < 5:
            # Write differences to file
            f.write(f"Original Text: {text}\n")
            f.write(f"Custom Tokens: {custom_tokenizer(text)}\n")
            f.write(f"NLTK Tokens: {word_tokenize(text)}\n")
            f.write("-" * 20 + "\n")
            count += 1

    # Write the summary paragraph to the file
    summary = """
    The custom tokenizer and NLTK's word tokenizer exhibit subtle differences in tokenization.
    The custom tokenizer primarily focuses on separating hashtags, mentions, URLs, and emojis, while NLTK's tokenizer provides a more nuanced approach to handling punctuation, contractions, and special characters.
    These variations highlight the flexibility and customization that building a custom tokenizer offers.
    """
    f.write(summary)

print("Tokenizer comparison results written to tokenizer_comparison.txt")