# -*- coding: utf-8 -*-
"""HW 4 C

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/100ZqFBq9Jpde_AljAUhO4rmcZ62YpTvF
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("crowdflower/twitter-airline-sentiment")

print("Path to dataset files:", path)

!pip install emojis

!pip install emoji

import pandas as pd
df = pd.read_csv(path + "/Tweets.csv")
print(df.head())

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import re
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import emoji
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('punkt_tab')

def clean_text(text):
    # 1. Remove mentions
    text = re.sub(r'@\w+', '', text)

    # 2. Remove currency symbols and amounts
    text = re.sub(r'\$\d+(?:\.\d+)?', '', text)

    # 3. Remove email addresses
    text = re.sub(r'\w+@\w+\.\w+', '', text)

    # 4. Remove emojis
    text = re.sub(r'[^\x00-\x7F]+', '', text)

    # 5. Replace HTML escaped characters
    text = re.sub(r'&lt;', '<', text)
    text = re.sub(r'&gt;', '>', text)
    text = re.sub(r'&amp;', '&', text)

    # 6. Normalize punctuation
    text = re.sub(r'[!?.]+', '.', text)

    # 7. Remove times and dates
    text = re.sub(r'\d{1,2}/\d{1,2}(?:/\d{2,4})? (?:\d{1,2}:\d{2}(?:[ap]m)?)?', '', text)
    text = re.sub(r'\d{1,2}:\d{2}(?:[ap]m)?', '', text)

    # 8. Remove URLs
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)

    # 9. Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    # 10. Convert to lowercase
    text = text.lower()

    # 11. Remove hashtags but keep the word
    text = re.sub(r'#(\w+)', r'\1', text)

    # 12. Remove numbers
    text = re.sub(r'\d+', '', text)

    # 13. Remove special characters except for punctuation
    text = re.sub(r'[^\w\s.,;!?-]', '', text)

    # 14. Replace multiple punctuation with single
    text = re.sub(r'([.,;!?-])+', r'\1', text)

    # 15. Remove leading/trailing punctuation
    text = text.strip('.,;!?-')

    # 16. Verb lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = nltk.word_tokenize(text)
    text = ' '.join([lemmatizer.lemmatize(token, 'v') for token in tokens])

    return text

# Apply cleaning to the 'text' column
df['cleaned_text'] = df['text'].apply(clean_text)

# Remove duplicate rows
df = df.drop_duplicates(subset=['cleaned_text', 'airline_sentiment'])

# Remove rows with empty tweets
df = df[df['cleaned_text'] != '']

X = df['cleaned_text']
y = df['airline_sentiment']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=3, shuffle=True)

# Print class distribution
print("Training set class distribution:", y_train.value_counts(normalize=True))
print("Test set class distribution:", y_test.value_counts(normalize=True))

# Create TF-IDF vectors
def encode_text(X_train, X_test):
  vectorizer = TfidfVectorizer()
  X_train_tfidf = vectorizer.fit_transform(X_train)
  X_test_tfidf = vectorizer.transform(X_test)
  return X_train_tfidf, X_test_tfidf, vectorizer

# Create and train the SVM classifier
svm_classifier = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, max_iter=100, tol=None, shuffle=True, random_state=3)
X_train_tfidf, X_test_tfidf, vectorizer = encode_text(X_train, X_test)
svm_classifier.fit(X_train_tfidf, y_train)

# Perform cross-validation
cv_scores = cross_val_score(svm_classifier, X_train_tfidf, y_train, cv=10)
validation_accuracy = cv_scores.mean()
print("Validation Accuracy:", validation_accuracy)

# Evaluate on the test set
y_pred = svm_classifier.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)
confusion = confusion_matrix(y_test, y_pred)

print("Test Accuracy:", accuracy)
print("Classification Report:\n", report)
print("Confusion Matrix:\n", confusion)

def ablation_study(X_train, y_train, X_test):
    def clean_text_with_lemmatization(text):
        lemmatizer = WordNetLemmatizer()
        tokens = word_tokenize(text)
        return " ".join([lemmatizer.lemmatize(token, pos='v') for token in tokens])

    def clean_text_with_emoji_handling(text):
        return emoji.demojize(text)

    def clean_text_with_all_steps(text):
        text = re.sub(r"@\w+", "", text)
        text = re.sub(r"\$\d+(?:\.\d+)?", "", text)
        text = re.sub(r"https?://\S+|www\.\S+", "", text)
        text = text.lower()
        text = emoji.demojize(text)
        return text

    def clean_text_with_lemmatization_and_emoji(text):
        text = emoji.demojize(text)
        lemmatizer = WordNetLemmatizer()
        tokens = word_tokenize(text)
        return " ".join([lemmatizer.lemmatize(token, pos='v') for token in tokens])

    def clean_text_with_emoji_and_lemmatization(text):
        lemmatizer = WordNetLemmatizer()
        tokens = word_tokenize(emoji.demojize(text))
        return " ".join([lemmatizer.lemmatize(token, pos='v') for token in tokens])

    preprocessing_combinations = [
        ("Original", lambda text: text),  # No preprocessing
        ("Lemmatization", lambda text: clean_text_with_lemmatization(text)),
        ("Emoji Handling", lambda text: clean_text_with_emoji_handling(text)),
        ("All", lambda text: clean_text_with_all_steps(text)),
        ("Lemmatization + Emoji Handling", lambda text: clean_text_with_lemmatization_and_emoji(text)),
        ("Emoji Handling + Lemmatization", lambda text: clean_text_with_emoji_and_lemmatization(text)),
    ]

    best_accuracy = 0
    best_combination = None

    # Run cross-validation for each combination
    for name, preprocess_func in preprocessing_combinations:
        # Apply the corresponding preprocessing
        X_train_preprocessed = X_train.apply(preprocess_func)

        X_train_tfidf, X_test_tfidf, vectorizer = encode_text(X_train_preprocessed, X_test.apply(preprocess_func))

        # Train and evaluate using 10-fold cross-validation
        svm = SGDClassifier(loss="hinge", penalty="l2", alpha=1e-4, max_iter=100, shuffle=True, random_state=3, tol=None)
        cv_scores = cross_val_score(svm, X_train_tfidf, y_train, cv=10, scoring="accuracy")

        mean_accuracy = cv_scores.mean()
        print(f"Accuracy for {name} preprocessing: {mean_accuracy:.4f}")

        # Update best preprocessing if necessary
        if mean_accuracy > best_accuracy:
            best_accuracy = mean_accuracy
            best_combination = name

    print(f"\nBest preprocessing combination: {best_combination} with accuracy: {best_accuracy:.4f}")

    return best_combination

print("\nStarting Ablation Study...")
best_preprocessing_combination = ablation_study(X_train, y_train, X_test)