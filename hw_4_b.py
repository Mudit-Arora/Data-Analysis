# -*- coding: utf-8 -*-
"""HW 4 B

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1thXfLkjnto7NX6UsiZMevPSQsf5UaprI
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("crowdflower/twitter-airline-sentiment")

print("Path to dataset files:", path)

import pandas as pd
df = pd.read_csv(path + "/Tweets.csv")
print(df.head())

import nltk
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
from nltk.stem import WordNetLemmatizer
import re
nltk.download('punkt_tab')

def clean_text(text):
    # 1. Remove mentions
    text = re.sub(r'@\w+', '', text)

    # 2. Remove currency symbols and amounts
    text = re.sub(r'\$\d+(?:\.\d+)?', '', text)

    # 3. Remove email addresses
    text = re.sub(r'\w+@\w+\.\w+', '', text)

    # 4. Remove emojis
    text = re.sub(r'[^\x00-\x7F]+', '', text)

    # 5. Replace HTML escaped characters
    text = re.sub(r'&lt;', '<', text)
    text = re.sub(r'&gt;', '>', text)
    text = re.sub(r'&amp;', '&', text)

    # 6. Normalize punctuation
    text = re.sub(r'[!?.]+', '.', text)

    # 7. Remove times and dates
    text = re.sub(r'\d{1,2}/\d{1,2}(?:/\d{2,4})? (?:\d{1,2}:\d{2}(?:[ap]m)?)?', '', text)
    text = re.sub(r'\d{1,2}:\d{2}(?:[ap]m)?', '', text)

    # 8. Remove URLs
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)

    # 9. Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    # 10. Convert to lowercase
    text = text.lower()

    # 11. Remove hashtags but keep the word
    text = re.sub(r'#(\w+)', r'\1', text)

    # 12. Remove numbers
    text = re.sub(r'\d+', '', text)

    # 13. Remove special characters except for punctuation
    text = re.sub(r'[^\w\s.,;!?-]', '', text)

    # 14. Replace multiple punctuation with single
    text = re.sub(r'([.,;!?-])+', r'\1', text)

    # 15. Remove leading/trailing punctuation
    text = text.strip('.,;!?-')

    # 16. Verb lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = nltk.word_tokenize(text)
    text = ' '.join([lemmatizer.lemmatize(token, 'v') for token in tokens])

    return text

# Apply cleaning to the 'text' column
df['cleaned_text'] = df['text'].apply(clean_text)

# Remove duplicate rows
df = df.drop_duplicates(subset=['cleaned_text', 'airline_sentiment'])

# Remove rows with empty tweets
df = df[df['cleaned_text'] != '']

print(df.head())